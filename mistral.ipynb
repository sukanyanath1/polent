{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sukan\\PycharmProjects\\Polent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Necessary Libraries\n",
    "# Ensure you have the required libraries installed:\n",
    "# !pip install transformers accelerate bitsandbytes\n",
    "\n",
    "# Step 2: Set Up BitsAndBytes Configuration\n",
    "# Configure the model to load in 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.50s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Load the Mistral 7B Model and Tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define the Prompt for Directed Sentiment Analysis\n",
    "def create_sentiment_prompt(sentence, person_a, person_b):\n",
    "    prompt = (\n",
    "        f\"Analyze the sentiment between {person_a} and {person_b} in the following sentence:\\n\"\n",
    "        f\"\\\"{sentence}\\\"\\n\"\n",
    "        f\"Please specify if the sentiment is positive, negative, or neutral.\"\n",
    "        f\"If the sentiment goes from {person_a} towards {person_b}, specify 1 otherwise 0. \"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define the Prompt for Directed Sentiment Analysis\n",
    "def create_sentiment_prompt(sentence, person_a, person_b):\n",
    "    prompt = (\n",
    "        f\"You are given a sentence where a person has directed sentiment towards another person\"\n",
    "        f\"\\\"{sentence}\\\"\\n\"\n",
    "        f\"Please specify: \\\"\\n0 if the sentiment between {person_a} and {person_b} is neutral.\"\n",
    "        f\"\\\"\\n 1 if {person_a} feels positively about {person_b}\"\n",
    "        f\"\\\"\\n 2 if {person_b} feels positively about {person_a}\"\n",
    "        f\"\\\"\\n 3 if {person_a} feels negatively about {person_b}\"\n",
    "        f\"\\\"\\n 4 if {person_b} feels negatively about {person_a}\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Generate Sentiment Analysis Using the Model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "def analyze_sentiment(sentence, person_a, person_b):\n",
    "    prompt = create_sentiment_prompt(sentence, person_a, person_b)\n",
    "    response = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    sentiment = response[0]['generated_text'].strip()\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment expressed by Newsom towards Trump: You are given a sentence where a person has directed sentiment towards another person\"That early morning call marked a new phase in the collaborative relationship that [Newsom] has built with [Trump] behind the scenes\"\n",
      "Please specify: \"\n",
      "0 if the sentiment between Newsom and Trump is neutral.\"\n",
      " 1 if Newsom feels positively about Trump\"\n",
      " 2 if Trump feels positively about Newsom\"\n",
      " 3 if Newsom feels negatively about Trump\"\n",
      " 4 if Trump feels negatively about Newsom\"\n",
      " The sentiment in this sentence is not specified. However, the phrase \"a new phase in the collaborative relationship\" implies that there is some level of positive sentiment between Newsom and Trump. So, I would classify this as a sentiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence = \"That early morning call marked a new phase in the collaborative relationship that [Newsom] has built with [Trump] behind the scenes\"\n",
    "person_a = \"Newsom\"\n",
    "person_b = \"Trump\"\n",
    "\n",
    "sentiment = analyze_sentiment(sentence, person_a, person_b)\n",
    "print(f\"Sentiment expressed by {person_a} towards {person_b}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
